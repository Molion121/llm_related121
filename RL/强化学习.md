# 强化学习

学习链接：https://blog.csdn.net/v_JULY_v/article/details/128965854

## 基本概念

期望：

对于连续分布：对于p（x）和f（x）乘积做积分

对于离散分布：对于p（x）和f（x）乘积做连加

agent 超级玛丽人物

state 环境状态，当前帧

action 人物运动

reward 奖励，赢的话加最多分（要比吃金币分多很多），吃金币加分

trajectory （state，action，reward）组成的s1,a1,r1 ..... s',a',r'

### 强化学习目标

RL其实是一个马尔科夫决策过程MDP，RL的目标是**最大化智能体策略在和动态环境交互过程中的价值，而策略的价值可以等价转换成奖励函数的期望，即最大化累计下来的奖励期望**
最优策略 = arg maxE { [奖励函数(状态,动作)] }

### MDP、MRP

![image-20250310135127109](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250310135127109.png)

### 随机性

* 给定state ，action动作具有随机性（agent可能往上跳也可能往左右走）
* 状态转移具有随机性，给定state和action，环境根据状态转移函数随机生成新的状态（小怪做动作的随机性，可能往左走也可能往右）

### 方法

RL为得到最优策略从而获取最大化奖励，有

1.基于值函数的方法，通过求解一个状态或者状态下某个动作的估值为手段，从而寻找最佳的价值函数，找到价值函数后，再提取最佳策略
比如Q-learning、DQN等，适合离散的环境下，比如围棋和某些游戏领域
2.基于策略的方法，一般先进行策略评估，即对当前已经搜索到的策略函数进行估值，得到估值后，进行策略改进，不断重复这两步直至策略收敛

比如策略梯度法(policy gradient，简称PG)，适合连续动作的场景，比如机器人控制领域
以及Actor-Criti(一般被翻译为演员-评论家算法)，Actor学习参数化的策略即策略函数，Criti学习值函数用来评估状态-动作对，不过，Actor-Criti本质上是属于基于策略的算法，毕竟算法的目标是优化一个带参数的策略，只是会额外学习价值函数，从而帮助策略函数更好的学习

此外，还有对策略梯度算法的改进，比如TRPO算法、PPO算法，当然PPO算法也可称之为是一种Actor-Critic架构

## 马尔科夫奖励过程

### 状态转移矩阵

![image-20250310140905872](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250310140905872.png)

### return ：cumulative future reward

在马尔科夫过程的基础上加入奖励函数R和折扣因子γ就可以得到马尔科夫奖励过程MRP

奖励函数：指的是某个状态s的奖励R(s)，是指转移到该状态s时可以获得奖励的期望，有![R(s) = E[R_{t+1}|S_t = s]](https://latex.csdn.net/eq?R%28s%29%20%3D%20E%5BR_%7Bt&plus;1%7D%7CS_t%20%3D%20s%5D)
注意，有的书上奖励函数和下面回报公式中的![R_{t+1}](https://latex.csdn.net/eq?R_%7Bt&plus;1%7D)的下标![t+1](https://latex.csdn.net/eq?t&plus;1)写为![t](https://latex.csdn.net/eq?t)，其实严格来说，先有![t](https://latex.csdn.net/eq?t)时刻的状态/动作之后才有![t+1](https://latex.csdn.net/eq?t&plus;1)时刻的奖励

t时刻的return ，可用**![G](https://latex.csdn.net/eq?G)示当下即时奖励和所有持久奖励等一切奖励的加权和**(考虑到一般越往后某个状态给的回报率越低，也即奖励因子或折扣因子越小，用![\gamma](https://latex.csdn.net/eq?%5Cgamma)表示)

![image-20250310141441648](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250310141441648.png)
$$ {其他说法，用U代表G}
U_t=R_t+γR_{t+1}+γ^2R_{t+2}+γ^3R_{t+3}+...
$$
### 值函数

一个状态的期望回报就成为这个状态的价值，所有状态的价值就是价值函数![V(s) = E[G_t|S_t=s]](https://latex.csdn.net/eq?V%28s%29%20%3D%20E%5BG_t%7CS_t%3Ds%5D)

![image-20250310162946431](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250310162946431.png)

- 前半部分表示**当前状态得到的即时奖励**![E[R_{t+1}|S_t = s] = R(s)](https://latex.csdn.net/eq?E%5BR_%7Bt&plus;1%7D%7CS_t%20%3D%20s%5D%20%3D%20R%28s%29)
- 后半部分表示**当前状态得到的所有持久奖励**![\gamma E[V(S_{t+1})|S_t = s]](https://latex.csdn.net/eq?%5Cgamma%20E%5BV%28S_%7Bt&plus;1%7D%29%7CS_t%20%3D%20s%5D)，可以根据从状态![s](https://latex.csdn.net/eq?s)出发的转移概率得到『至于上述推导的最后一步，在于![E[G_{t+1}|S_t = s]](https://latex.csdn.net/eq?E%5BG_%7Bt&plus;1%7D%7CS_t%20%3D%20s%5D)等于![E[V(S_{t+1})|S_t = s)]](https://latex.csdn.net/eq?E%5BV%28S_%7Bt&plus;1%7D%29%7CS_t%20%3D%20s%29%5D)』

### 贝尔曼方程

因此可得贝尔曼方程

![image-20250310163657643](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250310163657643.png)

期望就是对应的概率与相应值的累加

使用贝尔曼方程针对状态量多一点就很难计算，而求解较大规模的马尔可夫奖励过程中的价值函数时，可以用的方法包括：动态规划、蒙特卡洛方法、时序差分(temporal difference，简称TD)方法

## 马尔科夫决策过程

### 状态转移矩阵

当给定当前状态![S_{t}](https://latex.csdn.net/eq?S_%7Bt%7D)(比如![S_t =s](https://latex.csdn.net/eq?S_t%20%3Ds))，以及当前采取的动作![A_t](https://latex.csdn.net/eq?A_t)(比如![A_t = a](https://latex.csdn.net/eq?A_t%20%3D%20a))，那么下一个状态![S_{t+1}](https://latex.csdn.net/eq?S_%7Bt&plus;1%7D)出现的概率，可由状态转移概率矩阵表示如下

![\begin{aligned}P_{ss'}^{a} &= P(S_{t+1}=s'|S_t =s,A_t = a) \\&= {}P(s'|s,a) \end{aligned}](https://latex.csdn.net/eq?%5Cbegin%7Baligned%7DP_%7Bss%27%7D%5E%7Ba%7D%20%26%3D%20P%28S_%7Bt&plus;1%7D%3Ds%27%7CS_t%20%3Ds%2CA_t%20%3D%20a%29%20%5C%5C%26%3D%20%7B%7DP%28s%27%7Cs%2Ca%29%20%5Cend%7Baligned%7D)

假定在当前状态和当前动作确定后，其对应的奖励则设为![R_{t+1} = r](https://latex.csdn.net/eq?R_%7Bt&plus;1%7D%20%3D%20r)，故sutton的RL一书中，给的状态转移概率矩阵类似为

![p(s',r|s,a) = P\left \{ S_{t+1} = s',R_{t+1} = r |S_t = s,A_t = a \right \}](https://latex.csdn.net/eq?p%28s%27%2Cr%7Cs%2Ca%29%20%3D%20P%5Cleft%20%5C%7B%20S_%7Bt&plus;1%7D%20%3D%20s%27%2CR_%7Bt&plus;1%7D%20%3D%20r%20%7CS_t%20%3D%20s%2CA_t%20%3D%20a%20%5Cright%20%5C%7D)

从而可得奖励函数即为

也就是计算在状态s采取动作a后，转移到下一个状态s‘，同时获得奖励r的概率，那对所有可能的下一个状态求和，同时还对所有可能的奖励r求和

![\begin{aligned}R(s,a) &= E[R_{t+1} | S_t = s,A_t = a] \\&=\sum_{r\in R}^{}r \sum_{s'\in S}^{}p(s',r|s,a) \end{aligned}](https://latex.csdn.net/eq?%5Cbegin%7Baligned%7DR%28s%2Ca%29%20%26%3D%20E%5BR_%7Bt&plus;1%7D%20%7C%20S_t%20%3D%20s%2CA_t%20%3D%20a%5D%20%5C%5C%26%3D%5Csum_%7Br%5Cin%20R%7D%5E%7B%7Dr%20%5Csum_%7Bs%27%5Cin%20S%7D%5E%7B%7Dp%28s%27%2Cr%7Cs%2Ca%29%20%5Cend%7Baligned%7D)

### 策略函数

policy function 策略函数

观测到状态s，函数π会输出每个动作的概率

![image-20250310135207929](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250310135207929.png)

### 值函数

有了动作因素，需要重新梳理价值函数

#### 状态价值函数

所有状态的价值就是价值函数![V(s) = E[G_t|S_t=s]](https://latex.csdn.net/eq?V%28s%29%20%3D%20E%5BG_t%7CS_t%3Ds%5D)

![\begin{aligned} V_{\pi}(s) &= E_\pi [G_t|S_t = s] \\& = E_\pi [R_{t+1} + \gamma G_{t+1} | S_t = s] \\& = E_\pi [R_{t+1} + \gamma V_\pi (S_{t+1}) | S_t = s] \end{aligned}](https://latex.csdn.net/eq?%5Cbegin%7Baligned%7D%20V_%7B%5Cpi%7D%28s%29%20%26%3D%20E_%5Cpi%20%5BG_t%7CS_t%20%3D%20s%5D%20%5C%5C%26%20%3D%20E_%5Cpi%20%5BR_%7Bt&plus;1%7D%20&plus;%20%5Cgamma%20G_%7Bt&plus;1%7D%20%7C%20S_t%20%3D%20s%5D%20%5C%5C%26%20%3D%20E_%5Cpi%20%5BR_%7Bt&plus;1%7D%20&plus;%20%5Cgamma%20V_%5Cpi%20%28S_%7Bt&plus;1%7D%29%20%7C%20S_t%20%3D%20s%5D%20%5Cend%7Baligned%7D)

#### 动作价值函数

对当前状态s依据策略π执行动作a得到的期望回报，就是相应的Q函数，进入某个状态要采取的最优动作可以通过Q函数得到

![\begin{aligned} Q_\pi (s,a) &= E_\pi [G_t | S_t=s,A_t = a] \\& = E_\pi [R_{t+1} + \gamma G_{t+1}| S_t=s,A_t = a] \\& = E_\pi [R_{t+1} + \gamma Q_\pi (S_{t+1},A_{t+1})| S_t=s,A_t = a] \end{aligned}](https://latex.csdn.net/eq?%5Cbegin%7Baligned%7D%20Q_%5Cpi%20%28s%2Ca%29%20%26%3D%20E_%5Cpi%20%5BG_t%20%7C%20S_t%3Ds%2CA_t%20%3D%20a%5D%20%5C%5C%26%20%3D%20E_%5Cpi%20%5BR_%7Bt&plus;1%7D%20&plus;%20%5Cgamma%20G_%7Bt&plus;1%7D%7C%20S_t%3Ds%2CA_t%20%3D%20a%5D%20%5C%5C%26%20%3D%20E_%5Cpi%20%5BR_%7Bt&plus;1%7D%20&plus;%20%5Cgamma%20Q_%5Cpi%20%28S_%7Bt&plus;1%7D%2CA_%7Bt&plus;1%7D%29%7C%20S_t%3Ds%2CA_t%20%3D%20a%5D%20%5Cend%7Baligned%7D)

#### 二者联系

在使用策略![\pi](https://latex.csdn.net/eq?%5Cpi)时，**状态![s](https://latex.csdn.net/eq?s)的价值等于在该状态下基于策略![\pi](https://latex.csdn.net/eq?%5Cpi)采取所有动作的概率与相应的价值相乘再求和的结果**

![V_{\pi}(s) = \sum_{a \in A}^{}\pi (a|s)Q_\pi (s,a)](https://latex.csdn.net/eq?V_%7B%5Cpi%7D%28s%29%20%3D%20%5Csum_%7Ba%20%5Cin%20A%7D%5E%7B%7D%5Cpi%20%28a%7Cs%29Q_%5Cpi%20%28s%2Ca%29)

而在使用策略π时，**在状态 s下采取动作a的价值等于当前奖励R(s,a)，加上经过衰减的所有可能的下一个状态的状态转移概率与相应的价值的乘积**

![Q_\pi (s,a) = R(s,a) + \gamma \sum_{s' \in S}^{}P(s'|s,a)V_\pi (s')](https://latex.csdn.net/eq?Q_%5Cpi%20%28s%2Ca%29%20%3D%20R%28s%2Ca%29%20&plus;%20%5Cgamma%20%5Csum_%7Bs%27%20%5Cin%20S%7D%5E%7B%7DP%28s%27%7Cs%2Ca%29V_%5Cpi%20%28s%27%29)

### 贝尔曼方程

上式分别带入可得马尔科夫决策的贝尔曼方程

![V_{\pi}(s) = \sum_{a \in A}^{}\pi (a|s)\left [ R(s,a) + \gamma \sum_{s' \in S}^{}P(s'|s,a)V_\pi (s'))\right ]](https://latex.csdn.net/eq?V_%7B%5Cpi%7D%28s%29%20%3D%20%5Csum_%7Ba%20%5Cin%20A%7D%5E%7B%7D%5Cpi%20%28a%7Cs%29%5Cleft%20%5B%20R%28s%2Ca%29%20&plus;%20%5Cgamma%20%5Csum_%7Bs%27%20%5Cin%20S%7D%5E%7B%7DP%28s%27%7Cs%2Ca%29V_%5Cpi%20%28s%27%29%29%5Cright%20%5D)

![Q_\pi (s,a) = R(s,a) + \gamma \sum_{s' \in S}^{}P(s'|s,a)\left [ \sum_{a' \in A}^{}\pi (a'|s')Q_\pi (s',a') \right ]](https://latex.csdn.net/eq?Q_%5Cpi%20%28s%2Ca%29%20%3D%20R%28s%2Ca%29%20&plus;%20%5Cgamma%20%5Csum_%7Bs%27%20%5Cin%20S%7D%5E%7B%7DP%28s%27%7Cs%2Ca%29%5Cleft%20%5B%20%5Csum_%7Ba%27%20%5Cin%20A%7D%5E%7B%7D%5Cpi%20%28a%27%7Cs%27%29Q_%5Cpi%20%28s%27%2Ca%27%29%20%5Cright%20%5D)

![image-20250310194708266](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250310194708266.png)

## 动态规划

动态规划算法一般分为以下4个步骤：

1、描述最优解的结构
2、递归定义最优解的值
3、按自底向上的方式计算最优解的值   //此3步构成动态规划解的基础。
4、由计算出的结果构造一个最优解      //此步如果只要求计算最优解的值时，可省略
换言之，动态规划方法的最优化问题的俩个要素：最优子结构性质，和子问题重叠性质

最优子结构
如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质（即满足最优化原理）。意思就是，总问题包含很多个子问题，而这些子问题的解也是最优的。
重叠子问题
子问题重叠性质是指在用递归算法自顶向下对问题进行求解时，每次产生的子问题并不总是新问题，有些子问题会被重复计算多次。动态规划算法正是利用了这种子问题的重叠性质，对每一个子问题只计算一次，然后将其计算结果保存在一个表格中，当再次需要计算已经计算过的子问题时，只是在表格中简单地查看一下结果，从而获得较高的效率

例子：

n的函数f（n），第一次只跳一级，后面还剩n-1级，第一次跳2级，后面还剩n-2级台阶，同理可得当有三种跳法时，这种递归解法会占用很多的空间

![image-20250311154425844](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250311154425844.png)

递归写法：

![image-20250311155142395](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250311155142395.png)

递归树展开过程会有很多重复计算，相当于变成一棵树慢慢扎到底才知道值回滚

![image-20250311155047394](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250311155047394.png)

采用动态规划就可以一步步从树底爬上来

![image-20250311153839180](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250311153839180.png)

## 估计状态价值函数

### 动态规划求解最优策略

如何求解最优策略π

1. 首先，最优策略可以通过最大化![q_\pi (s,a)](https://latex.csdn.net/eq?q_%5Cpi%20%28s%2Ca%29)找到
   ![Q_\pi (s,a) = R(s,a) + \gamma \sum_{s' \in S}^{}P(s'|s,a)V_\pi (s')](https://latex.csdn.net/eq?Q_%5Cpi%20%28s%2Ca%29%20%3D%20R%28s%2Ca%29%20&plus;%20%5Cgamma%20%5Csum_%7Bs%27%20%5Cin%20S%7D%5E%7B%7DP%28s%27%7Cs%2Ca%29V_%5Cpi%20%28s%27%29)

2. 当![a= argmax \left \{ Q_*(s,a) \right \}](https://latex.csdn.net/eq?a%3D%20argmax%20%5Cleft%20%5C%7B%20Q_*%28s%2Ca%29%20%5Cright%20%5C%7D)时，
   $$
   \pi _*(a|s) = 1
   $$



根据前面的**二者联系**部分的状态![s](https://latex.csdn.net/eq?s)的价值等于在该状态下基于策略![\pi](https://latex.csdn.net/eq?%5Cpi)采取所有动作的概率与相应的价值相乘再求和的结果

现在采取所有动作的概率为1，也就是在最优策略π下对应的状态s的价值等于动作价值函数
$$
V_{\pi}(s) = Q_\pi (s,a)
$$
![v_{*}(s) = max \left \{ R(s,a) + \gamma \sum_{s' \in S}^{}P(s'|s,a)V_\pi (s')) \right \}](https://latex.csdn.net/eq?v_%7B*%7D%28s%29%20%3D%20max%20%5Cleft%20%5C%7B%20R%28s%2Ca%29%20&plus;%20%5Cgamma%20%5Csum_%7Bs%27%20%5Cin%20S%7D%5E%7B%7DP%28s%27%7Cs%2Ca%29V_%5Cpi%20%28s%27%29%29%20%5Cright%20%5C%7D)

因此可以得到满足贝尔曼最优方程的价值函数

![\begin{aligned} v_*(s) &= max E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t =s,A_t =a] \\&= max \sum_{s',r}^{}p(s',r|s,a) [r + \gamma v_*(s')] \end{aligned}](https://latex.csdn.net/eq?%5Cbegin%7Baligned%7D%20v_*%28s%29%20%26%3D%20max%20E%5BR_%7Bt&plus;1%7D%20&plus;%20%5Cgamma%20v_*%28S_%7Bt&plus;1%7D%29%20%7C%20S_t%20%3Ds%2CA_t%20%3Da%5D%20%5C%5C%26%3D%20max%20%5Csum_%7Bs%27%2Cr%7D%5E%7B%7Dp%28s%27%2Cr%7Cs%2Ca%29%20%5Br%20&plus;%20%5Cgamma%20v_*%28s%27%29%5D%20%5Cend%7Baligned%7D)

相当于当知道奖励函数和状态转换函数时，便可以根据下一个状态的价值来更新当前状态的价值，意味着可以把计算下一个可能状态的价值当成一个子问题，而把计算当前状态的价值看做当前问题，刚好就可以用DP来求解了

当前真实的下一个状态的价值是未知的，只能用当前的估计值来替代

### 蒙特卡洛法MC

通过大量的随机样本来估算或者近似真实值

我们可以用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值。考虑到 一个状态的价值是它的期望回报，那么如果我们用策略在MDP上采样很多条序列，然后计算从这个状态出发的回报再求其期望：

![V_\pi (s) = E_\pi [G_t|S_t = s] = \frac{1}{N} \sum_{i=1}^{N}G_{t}^{(i)}](https://latex.csdn.net/eq?V_%5Cpi%20%28s%29%20%3D%20E_%5Cpi%20%5BG_t%7CS_t%20%3D%20s%5D%20%3D%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7DG_%7Bt%7D%5E%7B%28i%29%7D)

### 时序差分法TD

一步一更新，或者多步更新，基于奖励和状态价值函数进行更新

![image-20250311163947261](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250311163947261.png)

### 区别：

![image-20250311163508251](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250311163508251.png)

MC就是一条路走到黑才能知道真实的回报，与我们预估的V（St）存在差距，我们通过学习率去缩小这个差距

MC是一条路走到黑然后再重新更新

TD是每走一段路就更新

DP是走完全部一段路再更新

![image-20250310221823235](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250310221823235.png)

## RL分类

基于模型的强化学习Model-based RL，可以使用动态规划求解，任务可定义为预测和控制，预测就是评估当前策略的好坏，即求解状态价值函数，控制的目的是寻找最优策略π和状态价值函数

在这里“模型”的含义是对环境进行建模，具体而言，是否已知其状态转移矩阵P和奖励函数R，即p（s’|s，a）和R（s，a）的取值

* 如果有对环境的建模，那么智能体便可以在执行动作前得知状态转移的情况即和奖励，也就不需要实际执行动作收集这些数据；

* 否则便需要进行采样，通过与环境的交互得到下一步的状态和奖励，然后仅依靠采样得到的数据更新策略
  

![image-20250310222036803](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250310222036803.png)

无模型的强化学习Model-free RL

* 基于价值的强化学习：其会学习并贪婪的选择值最大的动作，即![a = \underset{a}{\arg \max}\ Q(s,a)](https://latex.csdn.net/eq?a%20%3D%20%5Cunderset%7Ba%7D%7B%5Carg%20%5Cmax%7D%5C%20Q%28s%2Ca%29)，最经典的便是off-policy模式的Q-learning和on-policy模式的SARSA，一般得到的是确定性策略，off-policy解释：the learning is from the data **off** the **target policy**（引自《Reinforcement Learning An Introduction》）。也就是说RL算法中，将收集数据当做一个单独的任务，数据来源于一个单独的用于探索的策略(不是最终要求的策略)。on-policy--行为策略与目标策略相同。

* On/off-policy的概念帮助区分训练的数据来自于哪里。

* Off-policy方法中不一定非要采用重要性采样，要根据实际情况采用（比如，需要精确估计值函数时需要采用重要性采样；若是用于使值函数靠近最优值函数则不一定）。

* 基于策略的强化学习：对策略进行建模π（s，a）并优化，一般得到的是随机性策略

  ![image-20250311170434884](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250311170434884.png)

## 基于价值的强化学习

### Sarsa

对于无模型的RL问题，不知道目前的奖励函数R以及状态转移矩阵P，因此动作价值函数比状态价值函数更容易评估

如果用类似TD(0)控制的思路寻找最优的动作价值函数并提取出最优策略，便被称作Sarsa(0)算法，所以，Sarsa所做出的改变很简单，它将原本时序差分方法更新![V](https://latex.csdn.net/eq?V)的过程，变成了更新![Q](https://latex.csdn.net/eq?Q)，即可以如下表达

![Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t)]](https://latex.csdn.net/eq?Q%28S_t%2CA_t%29%20%5Cleftarrow%20Q%28S_t%2CA_t%29%20&plus;%20%5Calpha%20%5BR_%7Bt&plus;1%7D%20&plus;%20%5Cgamma%20Q%28S_%7Bt&plus;1%7D%2CA_%7Bt&plus;1%7D%29%20-%20Q%28S_t%2CA_t%29%5D)

![\gamma](https://latex.csdn.net/eq?%5Cgamma)的上标加![t+1](https://latex.csdn.net/eq?t&plus;1)即为![R](https://latex.csdn.net/eq?R)的下标，反过来，当最后一项![R](https://latex.csdn.net/eq?R)的下标![T](https://latex.csdn.net/eq?T)确定后，自然便可以得出![\gamma](https://latex.csdn.net/eq?%5Cgamma)的上标为![T -t -1](https://latex.csdn.net/eq?T%20-t%20-1))

![G_t = R_{t+1} + \gamma R_{t+2} + \gamma ^2 R_{t+3}+\cdots + \gamma ^{T-t-1}R_T](https://latex.csdn.net/eq?G_t%20%3D%20R_%7Bt&plus;1%7D%20&plus;%20%5Cgamma%20R_%7Bt&plus;2%7D%20&plus;%20%5Cgamma%20%5E2%20R_%7Bt&plus;3%7D&plus;%5Ccdots%20&plus;%20%5Cgamma%20%5E%7BT-t-1%7DR_T)

这里说的对于其他任意状态价值估计保持不变，是因为仅更新当前轨迹涉及的状态，未被采样的状态不会受到影响

![image-20250311174412205](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250311174412205.png)

### Q-learning

#### 同策略与异策略

行动遵循的行动策略与被评估的目标策略是同一个策略(如果要学习的智能体和与环境交互的智能体是相同的)，则称之为同策略，比如Sarsa
行动遵循的行动策略和被评估的目标策略是不同的策略(或如果要学习的智能体和与环境交互的智能体不是相同的)，则称之为异策略，比如Q-learning
异策略就是基于**重要性采样**的原理实现的(但反过来，*不是说只要采用了重要性采用，就一定是异策略*，比如下文将讲的PPO算法)，即通过使用另外一种分布，来逼近所求分布的一种方法

#### 重要性采样



### DQN

使用神经网络近似Q*

Deep Q-Network（DQN）

使用神经网络Q（s,a,w）近似Q*(s,a)

td算法使用梯度下降方法减小td error，使得真实值和预测值差距减小

不需要完成全程，通过掺入类似半程的真实数值，使得预测值不断靠近真实值

应用td算法到折扣回报后的Return：
$$
U_{t}=R_{t}+\gamma*U_{t+1}
$$
td算法应用到DQN：

在深度强化学习中有，也因此是为了让二者不断接近才需要反向传播梯度下降等
$$
Q(s_{t},a_{t};\omega) \approx r_{t}+\gamma*Q(s_{t+1},a_{t+1};\omega)
$$
1.观察当前时刻状态S{t}=s{t}和已经执行的动作A{t}=a{t}

2.用神经网络模拟DQN进行一次预测，输入是状态动作，输出是对动作的打分
$$
q_{t}=Q(S_{t},a_{t};\omega_{t})
$$
3.反向传播对w求导得到d_{t}

4.已经执行完动作后环境随机函数会提供新的状态S_{t+1}和reward rt

5.计算TD target 
$$
y_{t}=r_{t}+\gamma*\underset{a}{max}Q（s_{t+1},a;\omega_{t}）
$$
6.梯度下降
$$
w_{t+1}=w_{t}-\alpha*(q_{t}-y_{t})*d_{t}
$$




使用神经网络近似policy function

使用policy gradient算法

策略函数：
$$
\pi(a|s)
$$
输入是当前状态s

输出是给所有动作的概率值
$$
\pi(left|s)=0.2\qquad 
\pi(right|s)=0.1\qquad
\pi(up|s)=0.7
$$
由于游戏状态的多样式以及动作的多样式，显然只能通过policy network近似方法求得policy function

policy network

需满足有:其中A代表所有动作的集合，也就是对应所有动作的概率和为1
$$
\sum_{a\in\Alpha}{\pi(a|s;\theta)}=1
$$
策略梯度

对于离散动作，使用方式1：

针对每个动作分别求出其值再累加
$$
\dfrac{\partial V(s;\theta)}{\partial \theta}=\sum_a\dfrac{\partial\pi(a|s;\theta)}{\partial\theta}*Q_{\pi}(s,a)
$$
对于连续动作，使用方式2：A为连续变量，直接求期望需要对其定积分，但π已经近似为神经网络，对其进行积分过于复杂，需使用蒙特卡洛近似期望，
$$
\dfrac{\partial V(s;\theta)}{\partial\theta}=E_{A\sim\pi(*|s;\theta)}[\dfrac{\partial \log \pi(A|s;\theta)}{\partial \theta}*Q_{\pi}(s,A)]
$$
蒙特卡洛近似，随机抽取一个或多个动作
$$
g(\widehat{a},\theta)=\dfrac{\partial \log{\pi(\widehat{a}|s;\theta)}}{\part\theta}*Q_{\pi}(s,\widehat{a})
$$
使用g来近似policy gradient策略梯度



## actor-critic method

actor相当于运动员，策略网络控制agent运动，critic相当于裁判，给价值网络给动作打分，学习两个网络使得运动员分数高，裁判打分更精准
$$
V(s;\theta,\omega)=\sum_{a}{\pi}(a|s;\theta)*q(s,a;\omega)
\;\theta:策略网络参数\;\omega：价值网络参数
$$
大致流程：

1.观察状态s_{t}

2.根据policy function和给定s{t}随机抽取动作a_{t}

3.执行a_{t}观察新的状态s{t+1}和reward  r{t}

4.更新价值网络的参数通过td算法

5.更新策略网络的参数通过策略梯度

具体算法流程：

1、观察状态s{t}并根据policy function和给定s{t}随机抽取动作a{t}

2、执行动作a{t}观察新的状态s{t+1}和reward  r{t}

3、根据policy function和给定s{t+1}随机抽取动作a{t+1}（需注意每次只执行一次动作a{t}而不执行动作a{t+1}）

4、计算两次价值网络输出q（s{t}，a{t}；w{t}）和q（s{t+1}，a{t+1}；w{t}），用完就丢掉a{t+1}

5、计算TD error ：q{t}-(r{t}+权重*q{t+1})

6、反向传播对价值网络求导d w

7、更新价值网络（梯度下降）w{t+1}=w{t}-学习率*TD error d w

8、对策略网络反向传播求导d \theta

9、更新策略网络（梯度上升）t+1=t+学习率*td error d \theta（这里是使用baseline 效果更好）

最终目标是有良好的策略网络，训练完不需要critic裁判这一角色，价值网络只是作为打分者辅助actor往好的方向学习。



马尔科夫链：

具备马尔科夫性质的随机过程

马尔科夫性质是为了简化计算减小参数量，让未来时刻的状态仅与当前时刻状态有关，与以前的状态无关
$$
P(S_{t+1}|S_{t},S_{t-1},S_{t-2},...,S_{1})=P(S_{t+1}|S_{t})
$$




PPO算法：

训练策略以及对应的状态价值函数可以是共享参数的同一层，通过改变最后一层的输出即可，策略是要输出对应的动作概率，而状态价值函数则是输出一个价值，它是当前的时间步下到最终的时间步这一trajectory的reward进行逐步衰减叠加作为Label

用重要性采样来更新目标函数的梯度公式，通过观察参考模型的行为来调整自己的行为，类比就是θ’是好学生，θ是我们自己，要像好学生学习，但是这建立在参考的策略和训练的策略在同一情况下给出的动作概率分布差别不能太大的情况下，因为如果将坏学生和好学生类比学习实际学习不到什么

如何将二者分布相差限制，使用kl散度。kl散度是描述两个概率分布相似程度的量化指标

还有种实现是使用截断函数来替代kl散度

![image-20250227165555029](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250227165555029.png)

![image-20250227170201564](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250227170201564.png)



学习网站：

[强化学习极简入门：通俗理解MDP、DP MC TD和Q学习、策略梯度、PPO_强化学习入门-CSDN博客](https://blog.csdn.net/v_JULY_v/article/details/128965854)





## B站视频内容理解：

![image-20250222155507757](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250222155507757.png)

模仿学习

观察学习

迁移学习

imitation learning模仿学习

监督学习，就是样本都是标定好的，人类监督其结果，我们从X，Y中找出一个函数实现，比如图像分类任务，从对应的图像和label中找出一种映射关系，能够当输入一张图像时，得到相应的label

那如果是对于强化学习，对应的图像就是observation，观察，此时我们识别出类别是tiger，但是，我们是其他对应关系，会有相应的动作，比如逃跑、留下等，一般来说是概率为非0即1的，但是通常训练成概率分布，取最大概率的值，正常来说，observation仅仅是像素，计算机的值表示，但是结合物理要素以及理解，我们就能理解目前的state，也就是猎豹在追逐羚羊，但是观察有时候会受到影响但实际state是不变，比如因为车辆遮挡导致观察困难，

一般来说，可能无法从当前观察完美推理出状态，但从状态推出观察是可行的，状态包含生成观察所需的所有信息，过去不重要，知道现在才能帮助推出将来

![image-20250222162701216](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250222162701216.png)

![image-20250222163305408](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250222163305408.png)

控制领域可能会有的表示：成本函数和奖励函数也都是一个东西，**cost只是reward的负数**

![image-20250222163617230](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250222163617230.png)

分布式偏移：如果仅仅模仿学习，不能很好的弥补测试集和训练集之间的差距，当到模型未知的领域时，它会累加错误，不断犯错，我们定义一个函数当它正常执行就记为0，不正常情况则为1，我们需要做的是最小化犯错次数，对应也就是累加期望，一般来说最差的结果就是与T方成正比。



![image-20250222170731240](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250222170731240.png)

因果混淆causal confusion现象：如果存在更强相关的因果，会让策略无法集中推导出真实的原因，而把其他因果作为判断条件

如何使模型能够效果更佳：

1、通过数据增强方式，就像在自动驾驶中引入左右视野，达到增强数据能够在车子出现偏移知道该怎么做

2、使用更强大的模型使模型犯错误的概率变小，这样子总的犯错次数也会大大减小

3、多任务学习

4、改变算法DAgger

这些都是减小错误概率使得分布偏移问题减小：

非马尔科夫链可以通过使用历史记录来解决

多任务学习：

这是通过使训练数据与测试数据之前的分布一致

DAgger算法：旨在为模仿学习中的分布偏移问题提供一个更为原则性的解决方案

部分可观察的马尔科夫决策过程POMDP：

通常基于观察做出决策，而无法直接访问真实状态

![image-20250224155529967](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250224155529967.png)

强化学习的目标：

（有限时域）

τ表示轨迹，从s1->sT,s1表示初试状态，π代表策略，在当前的状态下执行动作的概率，而p代表转移概率，从当前的动作和状态转向下一个状态

最终是要使轨迹上的奖励总和期望值最大化，针对不同的策略参数即深度学习参数θ

![image-20250224160721500](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250224160721500.png)

如果将状态和动作联合仍会满足马尔科夫性质，转移概率是对应的转移概率以及策略的乘积

![image-20250224162243300](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250224162243300.png)

相应的，期望也能进行变换

![image-20250224163224336](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250224163224336.png)

如果对于无限时域

一种是平稳分布，这要求有个前提就是遍历性和非周期性，能够使马尔科夫链到达任意位置，同时还不是周期性的，满足这两个后当时间趋于无穷时，平稳分布指的就是经过转移矩阵，仍然得到一样的动作和状态，也就是保持不变u=Tu，当时间趋于无穷而状态恒等不变时，就会使得最终的期望等于恒等状态动作见第一条公式

![image-20250224175055087](C:\Users\92809\AppData\Roaming\Typora\typora-user-images\image-20250224175055087.png)
